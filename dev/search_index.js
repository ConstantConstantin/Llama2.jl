var documenterSearchIndex = {"docs":
[{"location":"devs/#Developer's-Corner","page":"Developer's Corner","title":"Developer's Corner","text":"You want to understand how this package works or modify the code you are running? Here the necessary tools are provided and explained.","category":"section"},{"location":"devs/#Tokenizer","page":"Developer's Corner","title":"Tokenizer","text":"","category":"section"},{"location":"devs/#Transformer","page":"Developer's Corner","title":"Transformer","text":"","category":"section"},{"location":"devs/#forward!","page":"Developer's Corner","title":"forward!","text":"","category":"section"},{"location":"devs/#Llama2.Tokenizer","page":"Developer's Corner","title":"Llama2.Tokenizer","text":"Tokenizer\n\nConstruct a tokenizer storing vocabulary entries, scores, and byte-piece mappings.\n\nConstructors\n\nTokenizer(vocab, vocab_scores, sorted_vocab, vocab_size, max_token_length, byte_pieces)   Construct a tokenizer directly from the provided fields.   Validate that max_token_length > 0 and that byte_pieces has length 256.\nTokenizer(path::String, vocab_size::Integer)   Load a tokenizer from a binary file.\n\nFields\n\nvocab: Token string sequences.  \nvocab_scores: Scores for each token.  \nsorted_vocab: Sorted token indices.  \nvocab_size: Number of vocabulary entries.  \nmax_token_length: Maximum token length in bytes.  \nbyte_pieces: Byte mapping (length 256).\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TokenIndex","page":"Developer's Corner","title":"Llama2.TokenIndex","text":"TokenIndex(str::String, id::Integer)\n\nCreate a TokenIndex from str and the identifier id.\n\nThe byte sequence is converted to String and the ID is converted to Int16.   Throw a DomainError if id ≤ 0.\n\nExamples\n\njulia> Llama2.TokenIndex(\"Julia\", 1)\nLlama2.TokenIndex(\"Julia\", 1)\n\njulia> Llama2.TokenIndex(\"Julia\", -1)\nERROR: DomainError with Token index must be > 0.\n[...]\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.compare_tokens","page":"Developer's Corner","title":"Llama2.compare_tokens","text":"compare_tokens(first_token::TokenIndex, second_token::TokenIndex) -> Bool\n\nCompare two first_token and scond_token by their string values. It returns true if first_token's string is lexicographically less than the second's, and false otherwise.\n\nExamples\n\njulia> Llama2.compare_tokens(Llama2.TokenIndex(\"A\", 1), Llama2.TokenIndex(\"B\", 2))\ntrue\n\njulia> Llama2.compare_tokens(Llama2.TokenIndex(\"B\", 1), Llama2.TokenIndex(\"A\", 2))\nfalse\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.str_lookup","page":"Developer's Corner","title":"Llama2.str_lookup","text":"str_lookup(str::String, sorted_vocab::Vector{TokenIndex}) -> Int16\n\nSearch for str within a sorted vocabulary sorted_vocab. If a match is found, it returns the corresponding token ID; otherwise, it returns -1. It uses a binary search for efficient lookup.\n\nExamples\n\njulia> Llama2.str_lookup(\"aa\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n1\n\njulia> Llama2.str_lookup(\"ba\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n-1\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.encode","page":"Developer's Corner","title":"Llama2.encode","text":"encode(tokenizer::Tokenizer, text::String)\n\nConverts text into a sequence of token IDs using tokenizer. First ensure the tokenizer's vocabulary is sorted, then encode each character into its corresponding ID. After that, iteratively merge token pairs with the highest scores to form longer tokens until no more merges are possible. Return the final token ID sequence.\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.Transformer","page":"Developer's Corner","title":"Llama2.Transformer","text":"Transformer(path::String)\n\nLoad a binary file with location path and construct a Transformer from its content. The file is expected to have a header of 7 Int32 values followed by Float32 data.\n\nExample\n\njulia> t = Llama2.Transformer(\"/PATH/TO/YOUR.bin\");\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.Config","page":"Developer's Corner","title":"Llama2.Config","text":"Config\n\nCreate a Config containing 7 Int32. These describe meta-data to read values from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TransformerWeights","page":"Developer's Corner","title":"Llama2.TransformerWeights","text":"TransformerWeights\n\nCreate a TransformerWeights containing several Float32 containers. These describe actual weight data that is loaded from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.RunState","page":"Developer's Corner","title":"Llama2.RunState","text":"RunState\n\nCreate a RunState containing several Float32 containers. These reflect the state of the Transformer at run-time.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.rmsnorm","page":"Developer's Corner","title":"Llama2.rmsnorm","text":"rmsnorm(x, w)\n\nCalculate the rmsnorm of x and w, the scaled product 'λw * x'.\n\nExamples\n\njulia>  x = [1.0f0,2,3];\n\njulia>  w = [1.0f0,1,1];\n\njulia> o = Llama2.rmsnorm(x, w) \n3-element Vector{Float32}:\n 0.46290955\n 0.9258191\n 1.3887286\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.softmax!","page":"Developer's Corner","title":"Llama2.softmax!","text":"softmax!(x) Updates the Output of an Layer 'x' with the softmax of the input.\n\nExamples\n\njulia> x = [-1.0f0,0,1];\n\njulia> Llama2.softmax!(x);\n\njulia> x\n3-element Vector{Float32}:\n 0.09003057\n 0.24472848\n 0.66524094\n\n\n\n\n\n","category":"function"},{"location":"#Llama2.jl","page":"Home","title":"Llama2.jl","text":"","category":"section"},{"location":"#What-is-Llama2?","page":"Home","title":"What is Llama2?","text":"LLama2 is a family of pre-trained LLMs by Meta AI. More information can be found at: https://www.llama.com/","category":"section"},{"location":"#What-is-Llama2.jl?","page":"Home","title":"What is Llama2.jl?","text":"Llama2.jl can inference a given model from within julia. For this cause you will have to provide your own model checkpoint. This project follows the procedure outlined by the run.c file from llama2.c.","category":"section"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"Start julia, activate a desired environment and add the package; it can then be loaded in your session:\n\n(@v1.11) pkg> activate .\n\n(myLlama2) pkg> add https://github.com/ConstantConstantin/Llama2.jl\n\njulia> using Llama2\n\n","category":"section"},{"location":"inference/#Inference","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"inference/#Prequisites","page":"Inference","title":"Prequisites","text":"A model checkpoint is required. You can use your own or e.g. get the example file provided by karpathy:\n\nwget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin","category":"section"},{"location":"inference/#Inferencing","page":"Inference","title":"Inferencing","text":"You can either generate a single text, optionally giving an input prompt, or have an interactive chat (TODO).","category":"section"},{"location":"inference/#Llama2.talktollm","page":"Inference","title":"Llama2.talktollm","text":"talktollm(modelpath::String, vocabpath::String, prompt::String, max_tokens::Int)\n\nGenerate text using a pretrained LLama2 transformer model. The function loads the model from modelpath and the corresponding tokenizer from vocabpath. It takes an initial  prompt string to start the text generation and generates up to max_tokens tokens.\n\njulia> talktollm(\"/PATH/TO/MODEL.bin\"\n                ,\"/PATH/TO/VOCAB.bin\"\n                ,\"hey whats up?\", 100);\n\n\n\n\n\n","category":"function"}]
}
