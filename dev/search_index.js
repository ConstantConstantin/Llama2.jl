var documenterSearchIndex = {"docs":
[{"location":"devs/#Developer's-Corner","page":"Developer's Corner","title":"Developer's Corner","text":"You want to understand how this package works or modify the code you are running? Here the necessary tools are provided and explained.","category":"section"},{"location":"devs/#Tokenizer","page":"Developer's Corner","title":"Tokenizer","text":"","category":"section"},{"location":"devs/#Transformer","page":"Developer's Corner","title":"Transformer","text":"","category":"section"},{"location":"devs/#forward!","page":"Developer's Corner","title":"forward!","text":"","category":"section"},{"location":"devs/#Llama2.Tokenizer","page":"Developer's Corner","title":"Llama2.Tokenizer","text":"Tokenizer\n\nConstruct a tokenizer storing vocabulary entries, scores, and byte-piece mappings.\n\nConstructors\n\nTokenizer(vocab, vocab_scores, sorted_vocab, vocab_size, max_token_length, byte_pieces)   Construct a tokenizer directly from the provided fields.   Validate that max_token_length > 0 and that byte_pieces has length 256.\nTokenizer(path::String, vocab_size::Integer)   Load a tokenizer from a binary file.\n\nFields\n\nvocab: Token string sequences.  \nvocab_scores: Scores for each token.  \nsorted_vocab: Sorted token indices.  \nvocab_size: Number of vocabulary entries.  \nmax_token_length: Maximum token length in bytes.  \nbyte_pieces: Byte mapping (length 256).\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TokenIndex","page":"Developer's Corner","title":"Llama2.TokenIndex","text":"TokenIndex(str::String, id::Integer)\n\nCreate a TokenIndex from str and the identifier id.\n\nThe byte sequence is converted to String and the ID is converted to Int16.   Throw a DomainError if id ≤ 0.\n\nExamples\n\njulia> Llama2.TokenIndex(\"Julia\", 1)\nLlama2.TokenIndex(\"Julia\", 1)\n\njulia> Llama2.TokenIndex(\"Julia\", -1)\nERROR: DomainError with Token index must be > 0.\n[...]\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.compare_tokens","page":"Developer's Corner","title":"Llama2.compare_tokens","text":"compare_tokens(first_token::TokenIndex, second_token::TokenIndex) -> Bool\n\nCompare two first_token and scond_token by their string values. It returns true if first_token's string is lexicographically less than the second's, and false otherwise.\n\nExamples\n\njulia> Llama2.compare_tokens(Llama2.TokenIndex(\"A\", 1), Llama2.TokenIndex(\"B\", 2))\ntrue\n\njulia> Llama2.compare_tokens(Llama2.TokenIndex(\"B\", 1), Llama2.TokenIndex(\"A\", 2))\nfalse\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.str_lookup","page":"Developer's Corner","title":"Llama2.str_lookup","text":"str_lookup(str::String, sorted_vocab::Vector{TokenIndex}) -> Int16\n\nSearch for str within a sorted vocabulary sorted_vocab. If a match is found, it returns the corresponding token ID; otherwise, it returns -1. It uses a binary search for efficient lookup.\n\nExamples\n\njulia> Llama2.str_lookup(\"aa\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n1\n\njulia> Llama2.str_lookup(\"ba\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n-1\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.encode","page":"Developer's Corner","title":"Llama2.encode","text":"encode(tokenizer::Tokenizer, text::String)\n\nConverts text into a sequence of token IDs using tokenizer. First ensure the tokenizer's vocabulary is sorted, then encode each character into its corresponding ID. After that, iteratively merge token pairs with the highest scores to form longer tokens until no more merges are possible. Return the final token ID sequence.\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.Transformer","page":"Developer's Corner","title":"Llama2.Transformer","text":"Transformer(path::String)\n\nLoad a binary file with location path and construct a Transformer from its content. The file is expected to have a header of 7 Int32 values followed by Float32 data.\n\nExample\n\njulia> t = Llama2.Transformer(\"/PATH/TO/YOUR.bin\");\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.Config","page":"Developer's Corner","title":"Llama2.Config","text":"Config\n\nCreate a Config containing 7 Int32. These describe meta-data to read values from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TransformerWeights","page":"Developer's Corner","title":"Llama2.TransformerWeights","text":"TransformerWeights\n\nCreate a TransformerWeights containing several Float32 containers. These describe actual weight data that is loaded from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.RunState","page":"Developer's Corner","title":"Llama2.RunState","text":"RunState\n\nCreate a RunState containing several Float32 containers. These reflect the state of the Transformer at run-time.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.rmsnorm","page":"Developer's Corner","title":"Llama2.rmsnorm","text":"rmsnorm(x, w)\n\nCalculate the rmsnorm of x and w, the scaled product 'λw * x'.\n\nExamples\n\njulia>  x = [1.0f0,2,3];\n\njulia>  w = [1.0f0,1,1];\n\njulia> o = Llama2.rmsnorm(x, w) \n3-element Vector{Float32}:\n 0.46290955\n 0.9258191\n 1.3887286\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.softmax!","page":"Developer's Corner","title":"Llama2.softmax!","text":"softmax!(x) Updates the Output of an Layer 'x' with the softmax of the input.\n\nExamples\n\njulia> x = [-1.0f0,0,1];\n\njulia> Llama2.softmax!(x);\n\njulia> x\n3-element Vector{Float32}:\n 0.09003057\n 0.24472848\n 0.66524094\n\n\n\n\n\n","category":"function"},{"location":"#Llama2.jl","page":"Home","title":"Llama2.jl","text":"","category":"section"},{"location":"#What-is-Llama2?","page":"Home","title":"What is Llama2?","text":"LLama2 is a family of pre-trained LLMs by Meta AI. More information can be found at: https://www.llama.com/","category":"section"},{"location":"#What-is-Llama2.jl?","page":"Home","title":"What is Llama2.jl?","text":"Llama2.jl can inference a given model from within julia. For this cause you will have to provide your own model checkpoint. This project follows the procedure outlined by the run.c file from llama2.c.","category":"section"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"Start julia, activate a desired environment and add the package:\n\n(@v1.11) pkg> activate .\n\n(myLlama2) pkg> add https://github.com/ConstantConstantin/Llama2.jl\n\nIn every subsequent session it can be loaded via:\n\njulia> using Llama2","category":"section"},{"location":"#Example-Usage","page":"Home","title":"Example Usage","text":"julia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\", \"In a small village \"))\nIn a small village house, there was a man named Tom. Tom was kind and would always shine his in front of the town. People from the village would come to look at Tom and feel happy.\nOne day, a little girl named Lily came to Tom. She did not have a passport. Tom saw Lily and said, \"Why don't you have a passport, Lily? Hop in and pass me a little in our country!\" Lily smiled and said, \"Yes, I feel comfortable when I am in my own nation!\"\nLily put on her sunglasses and they became good friends. The town was filled with happy puppies who shared their sunglasses with everyone. The people in the town knew that being kind and working together made everything better.\n\n","category":"section"},{"location":"inference/#Inference","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"inference/#Prequisites","page":"Inference","title":"Prequisites","text":"A model checkpoint is required. You can use your own or e.g. get the example file provided by karpathy:\n\nwget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin","category":"section"},{"location":"inference/#Inferencing","page":"Inference","title":"Inferencing","text":"You can either generate a single text, optionally giving an input prompt, or have an interactive chat (TODO).","category":"section"},{"location":"inference/#Llama2.talktollm","page":"Inference","title":"Llama2.talktollm","text":"talktollm(modelpath::String, [prompt::String]; max_tokens::Int, vocabpath::String, verbose::Bool)\n\nGenerate text using a pretrained LLama2 transformer model. Return that text as a String. Load the model from modelpath and the corresponding tokenizer from vocabpath (which defaults to \"data/tokenizer.bin\"). Take an initial prompt String to start the text generation and generate up to max_tokens tokens. If verbose, print the text during generation.\n\njulia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\"))\n Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, Lily was running and she fell and hit her head on a rock. She got a big ouchie and it started to bleed. \nLily's mom took her to the doctor and the doctor said she needed a stitch. Lily was scared, but her mom was very dependable and told her they would be coming back home soon. \nAfter the doctor fixed Lily's knee, they went home and Lily's friends came to play again. But Lily's mom noticed that she was playing with a ball and some new toys. This made her very happy.\n\njulia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\", \"\"What is this?\"\"))\n\"What is this?\" the woman asked.\nThe little girl looked at the bookion and said, \"This is a book about a princess. Maybe we can use it together.\"\nThey decided to sit down and read the book together. They read about a beautiful garden with lovely flowers. The little girl loved the book very much and said, \"I want to be a princess again!\"\n\"Maybe, if you read me another book,\" the woman said.\nFrom that day on, they would sit down and read the book every night before bed. They hoped that when they finished reading it, something magical would happen.\n\n\n\n\n\n","category":"function"}]
}
