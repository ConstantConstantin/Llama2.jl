var documenterSearchIndex = {"docs":
[{"location":"#Llama2","page":"Home","title":"Llama2","text":"Documentation for Llama2.\n\n","category":"section"},{"location":"#Llama2.TokenIndex","page":"Home","title":"Llama2.TokenIndex","text":"TokenIndex(str::AbstractVector{UInt8}, id::Integer)\n\nCreate a TokenIndex from a byte vector and an integer identifier.\n\nThe byte sequence is converted to Vector{UInt8} and the ID is converted to Int16.   Throw a DomainError if id â‰¤ 0.\n\nExamples\n\njulia> using Llama2;\n\njulia> TokenIndex([0x61], 1)\nTokenIndex(UInt8[0x61], 1)\n\njulia> TokenIndex([0x61], -1)\nERROR: DomainError with Token index must be > 0.\n[...]\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.Tokenizer","page":"Home","title":"Llama2.Tokenizer","text":"Tokenizer\n\nConstruct a tokenizer storing vocabulary entries, scores, and byte-piece mappings.\n\nConstructors\n\nTokenizer(vocab, vocab_scores, sorted_vocab, vocab_size, max_token_length, byte_pieces)   Construct a tokenizer directly from the provided fields.   Validate that max_token_length > 0 and that byte_pieces has length 256.\nTokenizer(path::String, vocab_size::Integer)   Load a tokenizer from a binary file.\n\nFields\n\nvocab: Token byte sequences.  \nvocab_scores: Scores for each token.  \nsorted_vocab: Sorted token indices.  \nvocab_size: Number of vocabulary entries.  \nmax_token_length: Maximum token length in bytes.  \nbyte_pieces: Byte mapping (length 256).\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.compare_tokens-Tuple{TokenIndex, TokenIndex}","page":"Home","title":"Llama2.compare_tokens","text":"compare_tokens(first_token::TokenIndex, second_token::TokenIndex) -> Bool\n\nCompare two TokenIndex objects for equality based solely on their byte-string contents.   Return true if both tokens contain identical str fields, regardless of ID.\n\nExamples\n\njulia> using Llama2;\n\njulia> compare_tokens(TokenIndex([0x61], 1), TokenIndex([0x61], 2))\ntrue\n\njulia> compare_tokens(TokenIndex([0x61], 1), TokenIndex([0x62], 1))\nfalse\n\n\n\n\n\n","category":"method"}]
}
